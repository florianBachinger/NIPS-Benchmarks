{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX or SymPy\n",
    "\n",
    "The currently supported, known constraints are defined as bounds on the image of first or second order derivatives of the function.\n",
    "Therefore, they encompass monotonic, or convexity/concavity constraints. \n",
    "\n",
    "For any trained prediction model, we might want to check if it adheres to the known constraints.\n",
    "For this, we can either symbolically derive the model using SymPy and analyze the image of its derivatives, alternatively, we can use JAX for automatic derivatives of native python functions.\n",
    "\n",
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "import SCRBenchmark.SRSDFeynman as srsdf\n",
    "from SCRBenchmark import Benchmark\n",
    "\n",
    "################ ICh6Eq20 ################\n",
    "ICh6Eq20 = Benchmark(srsdf.FeynmanICh6Eq20)\n",
    "\n",
    "def f(x):\n",
    "  return jnp.exp(-(x[0] / x[1]) ** 2 / 2) / (jnp.sqrt(2 * jnp.pi) * x[1])\n",
    "\n",
    "print(\"JAX ICh6Eq20 Test:\")\n",
    "print(ICh6Eq20.check_constraints(f, Library = \"JAX\"))\n",
    "\n",
    "print(\"SymPy ICh6Eq20 Test:\")\n",
    "print(ICh6Eq20.check_constraints(\"exp(-(x0 / x1) ** 2 / 2) / (sqrt(2 * pi) * x1)\"))\n",
    "\n",
    "\n",
    "################ ICh9Eq18 ################\n",
    "ICh9Eq18 = Benchmark(srsdf.FeynmanICh9Eq18)\n",
    "\n",
    "def g(x):\n",
    "  return srsdf.feynman.GRAVITATIONAL_CONSTANT * x[0] * x[1] / ((x[2] - x[3]) ** 2 + (x[4] - x[5]) ** 2 + (x[6] - x[7]) ** 2)\n",
    "\n",
    "print(\"JAX ICh9Eq18 Test:\")\n",
    "print(ICh9Eq18.check_constraints(g, Library = \"JAX\"))\n",
    "\n",
    "print(\"SymPy ICh9Eq18 Test:\")\n",
    "print(ICh9Eq18.check_constraints('6.67430e-11 * x0 * x1 / ((x2 - x3) ** 2 + (x4 - x5) ** 2 + (x6 - x7) ** 2)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX provides faster execution but less accurate (at higher precision) gradients.\n",
    "\n",
    "## Interchangeability\n",
    "The following code proves that, up to a certain precision**, both SymPy and JAX calculate the same gradients.\n",
    "Therefore, JAX serves as a suitable alternative for **(1)** models that cannot be derived symbolically, or **(2)** for quicker model evaluation during training (e.g., to guide the search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymPy Result:\n",
      "(True, [])\n",
      "JAX Result:\n",
      "(True, [])\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import sympy\n",
    "import SCRBenchmark.Constants.StringKeys as sk\n",
    "import SCRBenchmark.base as base\n",
    "\n",
    "\n",
    "\n",
    "import SCRBenchmark.SRSDFeynman as srsdf\n",
    "from SCRBenchmark import Benchmark\n",
    "ICh9Eq18 = Benchmark(srsdf.FeynmanICh9Eq18)\n",
    "\n",
    "########################## the actual equations once as string, once as JAX function ##########################\n",
    "expression = \" 6.67430e-11 * x0 * x1 / ((x2 - x3) ** 2 + (x4 - x5) ** 2 + (x6 - x7) ** 2)\"\n",
    "def f(x):\n",
    "  return srsdf.feynman.GRAVITATIONAL_CONSTANT * x[0] * x[1] / ((x[2] - x[3]) ** 2 + (x[4] - x[5]) ** 2 + (x[6] - x[7]) ** 2)\n",
    "\n",
    "\n",
    "########################## check constraints ##########################\n",
    "constraints = ICh9Eq18.get_constraints()\n",
    "\n",
    "constraints = [c for c in constraints if c[sk.EQUATION_CONSTRAINTS_DESCRIPTOR_KEY]!=sk.EQUATION_CONSTRAINTS_DESCRIPTOR_NO_CONSTRAINT]\n",
    "if(len(constraints) == 0):\n",
    "    print(\"no constraints\")\n",
    "\n",
    "if(ICh9Eq18.datasets is None):\n",
    "    ICh9Eq18.read_datasets_for_constraint_checking()\n",
    "\n",
    "########################## symbolic derivatives ##########################\n",
    "\n",
    "# replace the sympy local dictionary with the display names of variables if specified\n",
    "local_dict = ICh9Eq18.equation.get_sympy_eq_local_dict()\n",
    "\n",
    "# parse the provided candidate expression\n",
    "# will use display names if specified\n",
    "expr = sympy.parse_expr(expression, evaluate=False, local_dict= local_dict)\n",
    "\n",
    "#calculate all first order partial derivatives of the expression \n",
    "f_primes = [(sympy.Derivative(expr, var).doit(),var.name, 1) \n",
    "            for var\n",
    "            in local_dict.values()]\n",
    "\n",
    "#calculate all second order partial derivatives of the expression (every possible combination [Hessian])\n",
    "f_prime_mat = [[ (sympy.Derivative(f_prime, var).doit(), [prime_var_name,var.name], 2 ) \n",
    "                  for var\n",
    "                  in local_dict.values()] \n",
    "                for (f_prime, prime_var_name, _) \n",
    "                in f_primes]\n",
    "\n",
    "#flatten 2d Hessian to 1d list and combine them \n",
    "f_prime_mat_flattened = [item for sublist in f_prime_mat for item in sublist]\n",
    "derviatives = f_primes+f_prime_mat_flattened\n",
    "      \n",
    "\n",
    "########################## JAX derivatives ##########################\n",
    "\n",
    "# replace the sympy local dictionary with the display names of variables if specified\n",
    "var_names = [v.name for v in ICh9Eq18.equation.get_vars()]\n",
    "\n",
    "g = jax.jit(jax.grad(f))\n",
    "hessian = jax.jit(jax.hessian(f))\n",
    "\n",
    "########################## check constraints ##########################\n",
    "sympy_violated_constraints = []\n",
    "JAX_violated_constraints = []\n",
    "#check for all existing constraints if they are met\n",
    "for constraint in constraints:\n",
    "  #every constraint has a specific input range in which they apply\n",
    "  xs = ICh9Eq18.datasets[constraint[sk.EQUATION_CONSTRAINTS_ID_KEY]]\n",
    "\n",
    "  ########################## SymPy ##########################\n",
    "  matches = [ derivative for (derivative, var, _) in derviatives if var == constraint[sk.EQUATION_CONSTRAINTS_VAR_NAME_KEY]]\n",
    "  derivative = matches[0]\n",
    "  f = sympy.lambdify(local_dict.keys(), derivative, \"numpy\")\n",
    "  #calculate gradient per data point\n",
    "  # gradients = np.array([ f(*row) for row in xs ])\n",
    "  # speedup of 5:\n",
    "  f_v = np.vectorize(f)\n",
    "  gradients_sympy = f_v(*(xs.T))\n",
    "  descriptor_sympy = base.get_constraint_descriptor_for_gradients(gradients_sympy)\n",
    "\n",
    "  ########################## JAX ##########################\n",
    "  var_name_constraint = constraint[sk.EQUATION_CONSTRAINTS_VAR_NAME_KEY]\n",
    "  descriptor_JAX = sk.EQUATION_CONSTRAINTS_DESCRIPTOR_UNKOWN_CONSTRAINT\n",
    "\n",
    "  # checking the different types of constraints supported\n",
    "  if(constraint[sk.EQUATION_CONSTRAINTS_ORDER_DERIVATIVE_KEY] == 1):\n",
    "    #constraint is defined for the first order derivative\n",
    "    # the signs of the functions gradient are to be checked for the input domain\n",
    "    var_index = var_names.index(var_name_constraint)\n",
    "    gradients = y = jax.vmap(g)(xs) \n",
    "    var_gradients = gradients[:,var_index]\n",
    "    descriptor_JAX = base.get_constraint_descriptor_for_gradients(var_gradients)\n",
    "\n",
    "  elif(constraint[sk.EQUATION_CONSTRAINTS_ORDER_DERIVATIVE_KEY] == 2):\n",
    "    var1_index = var_names.index(var_name_constraint[0])\n",
    "    var2_index = var_names.index(var_name_constraint[1])\n",
    "    hessian_gradients = jax.vmap(hessian)(xs) \n",
    "    var_gradients = hessian_gradients[:,var1_index,var2_index]\n",
    "    descriptor_JAX = base.get_constraint_descriptor_for_gradients(var_gradients)\n",
    "\n",
    "\n",
    "  else:\n",
    "    raise \"constraint was available but it was not handled/checked\"\n",
    "\n",
    "  ############ gradients are almost equal ############\n",
    "  if( not np.allclose(gradients_sympy,var_gradients)):\n",
    "     print(\"Gradients do not match!\")\n",
    "     \n",
    "  if(descriptor_sympy != constraint[sk.EQUATION_CONSTRAINTS_DESCRIPTOR_KEY]):\n",
    "      sympy_violated_constraints.append(constraint)\n",
    "\n",
    "  if(descriptor_JAX != constraint[sk.EQUATION_CONSTRAINTS_DESCRIPTOR_KEY]):\n",
    "      JAX_violated_constraints.append(constraint)\n",
    "\n",
    "  \n",
    "print('SymPy Result:')\n",
    "print((len(sympy_violated_constraints) == 0, sympy_violated_constraints))\n",
    "\n",
    "print('JAX Result:')\n",
    "print((len(JAX_violated_constraints) == 0, JAX_violated_constraints))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NIPS-Benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
