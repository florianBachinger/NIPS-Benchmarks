{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SCRBenchmark import BenchmarkSuite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design of the SCRBenchmark Suite\n",
    "The notebook [generate_data.ipynb](./generate_data.ipynb) details how the `benchmark` class can be used to generate data for individual equation instances. Here, we detail how a full suite of benchmark sets can be generated and how we ensure repeatability and how we compare results.\n",
    "\n",
    "Generating the full suite is as easy as follows:\n",
    "```python\n",
    "from SCRBenchmark import BenchmarkSuite, FEYNMAN_SRSD_HARD,\n",
    "   HARD_NOISE_LEVELS,HARD_SAMPLE_SIZES\n",
    "BenchmarkSuite.create_hard_instances(target_folder='./Data'\n",
    "                                     , Equations=FEYNMAN_SRSD_HARD\n",
    "                                     , sample_sizes= HARD_SAMPLE_SIZES\n",
    "                                     , noise_levels=HARD_NOISE_LEVELS\n",
    "                                     , repetitions= 10 )\n",
    "```\n",
    "\n",
    "## Static Validation Sets\n",
    "Unlike the generation of training data, where we want data in a distribution similar to real-world occurrences, for the validation set we desire maximized coverage of the input domain to evaluate the extrapolation capabilities of the trained model. As guided behavior is one of the key benefits of shape-constrained regression and the introduction of prior knowledge in the form of shape constraints.\n",
    "\n",
    "To facilitate comparison of multiple training runs and different algorithms we reuse one static validation dataset for each equation. This data is generated by uniform sampling of a large size of data from the full defined input domain. The target is calculated by evaluating the known base equation on the input data, and we do not introduce artificial noise on the validation data. See [generate_data.ipynb](generate_data.ipynb) for more detailed information on the sampling methodology.\n",
    "\n",
    "The validation sets are shipped as fixed csv files in [`../SCRBenchmark/Data/Test`](../SCRBenchmark/Data/Test) with one file per equation.\n",
    "When we generate benchmark data, we sample training data from the log10 based distribution and simply append the contents of the responding validation set. \n",
    "\n",
    "## Seeded Training Data\n",
    "We provide a fixed range of seeds to ensure repeatability in sampling the training sets. \n",
    "Thereby, we ensure a fair benchmarking of algorithms. \n",
    "\n",
    "We sample new training data for each repetition. Therein, each repetition seeds `np.random.seed(xyz)` it's associated integer number.\n",
    "\n",
    "```python \n",
    "SEEDS = [\n",
    " 342229   ,1271677  ,571939   ,926645   ,2300754  , #...\n",
    " ,1676573  ,1623234  ,58404    ,1449071  ,1477615\n",
    "]\n",
    "\n",
    "#...\n",
    "def create_hard_instances( target_folder = './data',\n",
    "                              Equations = FEYNMAN_SRSD_HARD,\n",
    "                              sample_sizes = HARD_SAMPLE_SIZES,\n",
    "                              noise_levels = HARD_NOISE_LEVELS,\n",
    "                              repetitions = None):\n",
    "  #... iterate over equations, sample_sizes, selected noise_levels and repetitions\n",
    "  BenchmarkSuite.create_individual_dataset(target_folder,\n",
    "                                          benchmark,\n",
    "                                          equation_folder,\n",
    "                                          noise_level,\n",
    "                                          sample_size,\n",
    "                                          seed = SEEDS[repetition],\n",
    "                                          sampling_patience = 40,\n",
    "                                          )\n",
    "  #...\n",
    "```\n",
    "\n",
    "The seed is fixed before sampling data and sampling the random noise.\n",
    "```python\n",
    "def create_dataset(self, sample_size,  noise_level = 0, seed = None, patience = 10 ):\n",
    "        assert (0<=noise_level and noise_level<=1), f'noise_level must be in [0,1]'\n",
    "\n",
    "        # fixing seed before data and noise sampling if seed is provided\n",
    "        if(not (seed is None)):\n",
    "          np.random.seed(seed)\n",
    "\n",
    "        xs = self.equation.create_dataset(sample_size,patience)\n",
    "\n",
    "        if(noise_level>0):\n",
    "          std_dev = np.std(xs[:,-1])\n",
    "          xs[:,-1] = xs[:,-1] + np.random.normal(0,std_dev*np.sqrt(noise_level),len(xs))\n",
    "\n",
    "        return (xs, self.read_test_dataframe().to_numpy())\n",
    "```\n",
    "\n",
    "As long as the user selected the same adjustable parameters `noise_level, sample_size` they are guaranteed to receive the same resulting dataset.\n",
    "\n",
    "[../tests/check_benchmark_generation.py](../tests/check_benchmark_generation.py) asserts that a generating the dataset for a full benchmark suite is guaranteed to produce the same data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NIPS-Benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
